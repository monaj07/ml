{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sb\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import urllib\n",
    "#------------------\n",
    "sys.path.append(\"..\")\n",
    "from scripts.dataset import TimeSeriesDataset\n",
    "from scripts.models import MLR, MLP\n",
    "from scripts.resnet_family import resnet20_cifar\n",
    "from scripts.utils_cm import compute_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a synthetic time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_days = 31\n",
    "trend_slope = 15\n",
    "noise_coef = 2\n",
    "noise_coef_outlier = 0 * noise_coef\n",
    "\n",
    "x_period = np.linspace(0, 2*np.pi, 288)\n",
    "signal = []\n",
    "for _ in range(total_days):\n",
    "    y = 10 * np.sin(4 * x_period) * np.exp(-0.5 * x_period)\n",
    "    y[-100:] = 0\n",
    "    n = noise_coef * np.random.randn(len(x_period))\n",
    "    if np.random.choice(7) == 0:\n",
    "        n += noise_coef_outlier * np.random.randn(len(x_period))\n",
    "    y += n\n",
    "    signal.append(y)\n",
    "signal = np.hstack(signal)\n",
    "\n",
    "trend = trend_slope * np.linspace(0, 1, len(signal))\n",
    "signal += trend\n",
    "x = np.arange(len(signal))\n",
    "\n",
    "regressor = trend + np.cos(.0005 * x) + 0.1 * np.random.randn(len(trend))\n",
    "\n",
    "plt.plot(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl = STL(signal[:-288], period=288, seasonal=7)\n",
    "stl_result = stl.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal = stl_result.seasonal\n",
    "trend = stl_result.trend\n",
    "signal_train_detrend = signal[:-288] - trend\n",
    "plt.plot(signal_train_detrend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = signal_train_detrend\n",
    "train_data = np.hstack([seasonal.reshape(-1, 1)])\n",
    "test_labels = signal[-288:]\n",
    "test_data = np.hstack([seasonal[-288:].reshape(-1, 1)])\n",
    "\n",
    "test_trend = ((trend[-1] - trend[-288]) + trend[-288:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add external regressors to the feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.hstack([train_data, regressor[:-288].reshape(-1, 1)])\n",
    "# test_data = np.hstack([test_data, regressor[-288:].reshape(-1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TimeSeriesDataset(train_data, train_labels), batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(TimeSeriesDataset(test_data, test_labels), batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Training Model and Configuration](#Training-Model-and-Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [100, 100, 100, 100]\n",
    "dropout = 0.5\n",
    "lr=0.001\n",
    "tau = 1\n",
    "wd = 0.005 #  0.01**2 * (1 - dropout) / (2. * len(train_loader) * tau)\n",
    "\n",
    "model = MLR(input_size=train_data.shape[1], nclasses=1, hidden_layers=hidden_layers, dropout=dropout)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "model.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_vec = []\n",
    "reset_loss_every = 300\n",
    "n_epochs = 20\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for it, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data, target = batch\n",
    "        output = model(data.float().to(device))\n",
    "        loss = F.mse_loss(output, target.view(-1, 1).to(device))\n",
    "        loss_vec.extend([loss.item()])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (it % reset_loss_every) == 0:\n",
    "            print(f\"epoch: {epoch}, it: {it}, average_loss: {np.mean(loss_vec)}\")\n",
    "            loss_vec = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "ensemble_size = 1000\n",
    "\n",
    "loss_vec = []\n",
    "forecast = []\n",
    "gt = []\n",
    "for it, batch in enumerate(test_loader):\n",
    "    data, target = batch\n",
    "    output = []\n",
    "    for ensemble_it in range(ensemble_size):\n",
    "        output.append(model(data.float().to(device)).data.numpy())\n",
    "    output = np.hstack(output)\n",
    "    forecast.append(output)\n",
    "    gt.append(target.view(-1).data.numpy())\n",
    "    \n",
    "forecast = np.vstack(forecast)\n",
    "forecast += test_trend[:, np.newaxis]\n",
    "predictive_mean = forecast.mean(1)\n",
    "predictive_std = forecast.std(1)\n",
    "gt = np.hstack(gt)\n",
    "mse_loss = ((gt - predictive_mean)**2).mean()\n",
    "print(f\"Test mse_loss = {mse_loss}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "axes[0].plot(predictive_mean, label=\"Forecast\")\n",
    "axes[0].plot(gt, label=\"Ground Truth\")\n",
    "axes[0].legend()\n",
    "axes[1].plot(predictive_mean, color='r', label=\"Forecast\")\n",
    "axes[1].fill_between(np.arange(288), predictive_mean-2*predictive_std, predictive_mean+2*predictive_std, label='prediction interval', color='g', alpha=0.9);\n",
    "axes[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above training+evaluation experiment for two different values of DropourRate={0, 0.5} in the [Training-Model-and-Configuration](http://localhost:8888/notebooks/uncertainty_regression.ipynb#Training-Model-and-Configuration) cell (above) and see how that single change introduces the prediction interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
