{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "#----------------------\n",
    "from dataset import CreateDataBatches\n",
    "from models import Model\n",
    "from utils_cm import compute_cm, split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate synthetic data using Gaussians\n",
    "sizes = [1000, 1000]\n",
    "\n",
    "# class0:\n",
    "mu0 = np.array([-3, 5])\n",
    "cov0 = np.array([[10, 0], [0, 10]]) * 0.5\n",
    "data0 = np.random.multivariate_normal(mu0, cov0, size=sizes[0])\n",
    "\n",
    "# class1:\n",
    "mu1 = np.array([0, -3])\n",
    "cov1 = np.array([[20, -1], [-1, 20]]) * 0.5\n",
    "data1 = np.random.multivariate_normal(mu1, cov1, size=sizes[1])\n",
    "\n",
    "### Combine data from different classes, shuffle them and split it into train, validation and test sets\n",
    "data = np.vstack([data0, data1])\n",
    "labels = np.concatenate([i * np.ones(sizes[i]) for i in range(len(sizes))]).astype(int)\n",
    "N = sum(sizes)\n",
    "Ts = [int(v * N) for v in (0.6, 0.8, 1)]\n",
    "data_train, labels_train, data_val, labels_val, data_test, labels_test = split_dataset(data, labels, Ts)\n",
    "\n",
    "classes = np.unique(labels)\n",
    "class_colours = ['r', 'b']\n",
    "\n",
    "idx_train = [np.where(labels_train == c)[0] for c in classes]\n",
    "idx_test = [np.where(labels_test == c)[0] for c in classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classes:\n",
    "    plt.scatter(data_train[idx_train[c], 0], data_train[idx_train[c], 1], marker='.', s=100, color=class_colours[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(CreateDataBatches(data_train, labels_train), batch_size=16, shuffle=True)\n",
    "valloader = DataLoader(CreateDataBatches(data_val, labels_val), batch_size=16, shuffle=True)\n",
    "testloader = DataLoader(CreateDataBatches(data_test, labels_test), batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size=data_train.shape[-1], nclasses=len(classes), hidden_layers=[8], dropout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value = 0\n",
    "n_epochs = 1\n",
    "reset_loss_every = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for it, train_batch in enumerate(trainloader):\n",
    "        model.train()\n",
    "        train_data_batch, train_labels_batch = train_batch\n",
    "        output = model(train_data_batch.to(device).float())\n",
    "        optim.zero_grad()\n",
    "        loss = F.cross_entropy(output, train_labels_batch.to(device), reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        loss_value += loss.data.item()\n",
    "        optim.step()\n",
    "\n",
    "        if it % reset_loss_every == 0 and it > 0:\n",
    "            model.eval()\n",
    "            gt_val, preds_val = [], []\n",
    "            for it_val, val_batch in enumerate(valloader):\n",
    "                val_data_batch, val_labels_batch = val_batch\n",
    "                output_val = model(val_data_batch.to(device).float())\n",
    "                preds_val.append(F.softmax(output_val, dim=1).data.numpy().argmax(axis=1))\n",
    "                gt_val.append(val_labels_batch.numpy())\n",
    "            preds_val = np.hstack(preds_val)\n",
    "            gt_val = np.hstack(gt_val)\n",
    "            recall, precision = compute_cm(gt_val, preds_val, classes)\n",
    "            average_loss = np.round(loss_value / reset_loss_every, 4)\n",
    "            print(f'epoch: {epoch}, iteration: {it}, recall: {recall},  precision: {precision}, average_loss: {average_loss}')\n",
    "            loss_value = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify all grid points to visualize decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grid = np.array([(x0, x1) for x0 in np.arange(data_test[:, 0].min(), data_test[:, 0].max(), .1)\n",
    "                               for x1 in np.arange(data_test[:, 1].min(), data_test[:, 1].max(), .1)])\n",
    "data_grid_norm = data_grid if gbc.mean is None else (data_grid - gbc.mean) / gbc.std\n",
    "predictions_data_grid_gbc = gbc.predict(data_grid)\n",
    "idx_grid = [np.where(predictions_data_grid_gbc == c)[0] for c in classes]\n",
    "for c in classes:\n",
    "    ax[1].plot(data_grid_norm[idx_grid[c], 0], data_grid_norm[idx_grid[c], 1], '.', color=class_colours[c])\n",
    "# Visualize our own data points\n",
    "data_test_norm = data_test if gbc.mean is None else (data_test - gbc.mean) / gbc.std\n",
    "for c in classes:\n",
    "    ax[1].plot(data_test_norm[idx_test[c], 0], data_test_norm[idx_test[c], 1], 'o', markeredgecolor='k', color=class_colours[c])\n",
    "ax[1].set_title(\"Gradient Boosted Classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
