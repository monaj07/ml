{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sb\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import urllib\n",
    "#------------------\n",
    "sys.path.append(\"..\")\n",
    "from scripts.dataset import TimeSeriesDataset\n",
    "from scripts.models import MLR, MLP\n",
    "from scripts.resnet_family import resnet20_cifar\n",
    "from scripts.utils_cm import compute_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CO2 Concentration Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/yaringal/DropoutUncertaintyCaffeModels/tree/master/co2_regression/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"data/co2/co2_train.h5\", \"r\") as f:\n",
    "    keys = list(f.keys())\n",
    "    date = np.array(f[keys[0]]).reshape(-1, 1)\n",
    "    value = np.array(f[keys[1]]).reshape(-1, 1)\n",
    "    signal_train = pd.DataFrame(np.hstack([date, value]), columns=['date', 'value'])\n",
    "\n",
    "with h5py.File(\"data/co2/co2_test.h5\", \"r\") as f:\n",
    "    keys = list(f.keys())\n",
    "    date = np.array(f[keys[0]]).reshape(-1, 1)\n",
    "    value = np.array(f[keys[1]]).reshape(-1, 1) * 0\n",
    "    signal_test = pd.DataFrame(np.hstack([date, value]), columns=['date', 'value'])\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 6))\n",
    "sb.lineplot(ax=ax, x='date', y='value', data=signal_train, label='train')\n",
    "sb.lineplot(ax=ax, x='date', y='value', color='g', data=signal_test, label='test')\n",
    "ax.legend()\n",
    "# sb.lineplot(ax=ax[1], x='date', y='value', color='g', data=signal_test, label='test')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stl = STL(train_labels, period=12, seasonal=7)\n",
    "# stl_result = stl.fit()\n",
    "# stl_result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = signal_train.value\n",
    "train_data = np.hstack([signal_train.date.to_numpy().reshape(-1, 1)])\n",
    "test_labels = signal_test.value\n",
    "test_data = np.hstack([signal_test.date.to_numpy().reshape(-1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = train_data\n",
    "# test_labels = train_labels\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(TimeSeriesDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TimeSeriesDataset(test_data, test_labels), batch_size=batch_size, shuffle=False)\n",
    "plt.plot(train_data, train_labels)\n",
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Training Model and Configuration](#Training-Model-and-Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [100, 100, 100, 100]\n",
    "dropout = 0.1\n",
    "lr = 0.001\n",
    "tau = 1\n",
    "wd = 0.000005 #  0.01**2 * (1 - dropout) / (2. * len(train_loader) * tau)\n",
    "\n",
    "model = MLR(input_size=train_data.shape[1], nclasses=1, hidden_layers=hidden_layers, dropout=dropout, batch_norm=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "model.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "# # The above statement skips this cell\n",
    "\n",
    "model_file_name = \"checkpoint_co2_dataset_200k\"\n",
    "\n",
    "try:\n",
    "    model = torch.load(f'./weights/{model_file_name}.pth')['model']\n",
    "    print(\"Loaded the model\")\n",
    "except:  \n",
    "    reset_loss_every = 100\n",
    "    n_epochs = 200000\n",
    "    device = \"cpu\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    def set_bn_eval(module):\n",
    "        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n",
    "            module.eval()\n",
    "\n",
    "    for epoch in range(n_epochs+1):\n",
    "        loss_epoch = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        model.train()\n",
    "        for it, batch in enumerate(train_loader):\n",
    "            data, target = batch\n",
    "\n",
    "            if len(target) < batch_size:\n",
    "    #             print(f\"it: {it} -> eval mode for BN\")\n",
    "    #             model.apply(set_bn_eval)\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.float().to(device))\n",
    "            loss = F.mse_loss(output, target.view(-1, 1).to(device))\n",
    "            num_samples += len(target)\n",
    "            loss_epoch += loss.item() * len(target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"epoch: {epoch}, num_samples: {num_samples}, average_loss: {loss_epoch / num_samples}\")\n",
    "\n",
    "        if (epoch % (n_epochs//2)) == 0 and epoch > 0:\n",
    "            lr *= 0.5\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "    # Save the model and weights\n",
    "    checkpoint = {'model': model,\n",
    "                  'state_dict': model.state_dict(),\n",
    "                  'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(checkpoint, './weights/checkpoint_co2_dataset_200k.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable DropOut during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode_dropout(m):\n",
    "    if type(m) == torch.nn.Dropout:\n",
    "        m.train()\n",
    "model.eval()\n",
    "model.apply(train_mode_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "ensemble_size = 1000\n",
    "\n",
    "gt = []\n",
    "forecast = []\n",
    "for it, batch in enumerate(test_loader):\n",
    "    data, target = batch\n",
    "    output = []\n",
    "    for ensemble_it in range(ensemble_size):\n",
    "        output.append(model(data.float().to(device)).data.numpy())\n",
    "    output = np.hstack(output)\n",
    "    forecast.append(output)\n",
    "    gt.append(target.view(-1).data.numpy())\n",
    "    \n",
    "forecast = np.vstack(forecast)\n",
    "predictive_mean = forecast.mean(1)\n",
    "predictive_std = forecast.std(1)\n",
    "print(predictive_mean.shape)\n",
    "# print(predictive_std)\n",
    "gt = np.hstack(gt)\n",
    "mse_loss = ((gt - predictive_mean)**2).mean()\n",
    "# print(f\"Test mse_loss = {mse_loss}\")\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 6))\n",
    "axes.plot(train_data.squeeze(), train_labels.squeeze(), 'b', label=\"Train data\", alpha=0.8);\n",
    "axes.axvline(train_data.squeeze()[-1], color='g',linestyle='--');\n",
    "axes.plot(test_data.squeeze(), predictive_mean, color='r', label=\"Forecast\")\n",
    "axes.fill_between(test_data.squeeze(), predictive_mean-2*predictive_std, predictive_mean+2*predictive_std, label='prediction interval', color='g', alpha=0.9);\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above training+evaluation experiment for two different values of DropourRate={0, 0.1} in the [Training-Model-and-Configuration](http://localhost:8888/notebooks/uncertainty_regression.ipynb#Training-Model-and-Configuration) cell (above) and see how that single change introduces the prediction interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References: \n",
    "[What My Deep Model Doesn't Know](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)\n",
    "\n",
    "[Dropout as a Bayesian Approximation:\n",
    "Representing Model Uncertainty in Deep Learning](http://proceedings.mlr.press/v48/gal16.pdf)\n",
    "\n",
    "[Dropout as Regularization and Bayesian Approximation](https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
