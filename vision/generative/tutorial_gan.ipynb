{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=5, output_dim=2, hidden_sizes=(10, 5)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(latent_dim, hidden_sizes[0]),\n",
    "                       nn.BatchNorm1d(hidden_sizes[0]),\n",
    "                       nn.ReLU6(inplace=True)])\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.BatchNorm1d(hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU6(inplace=True))\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_dim))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        out = torch.sigmoid(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_sizes=(10, 5), dropout_p=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_dim, hidden_sizes[0]),\n",
    "                       nn.LeakyReLU(0.2, inplace=True),\n",
    "                       nn.Dropout(dropout_p)])\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Dropout(dropout_p))\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 2))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underlying distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [1000, 1000, 1000]\n",
    "mu0 = np.array([-13, 15])\n",
    "cov0 = np.array([[6, 5], [5, 6]])\n",
    "data0 = np.random.multivariate_normal(mu0, cov0, size=sizes[0])\n",
    "mu1 = np.array([0, -3])\n",
    "cov1 = np.array([[20, 0], [0, 1]])\n",
    "data1 = np.random.multivariate_normal(mu1, cov1, size=sizes[1])\n",
    "mu2 = np.array([10, 13])\n",
    "cov2 = np.array([[3, 0], [0, 3]])\n",
    "data2 = np.random.multivariate_normal(mu2, cov2, size=sizes[2])\n",
    "\n",
    "selected_data = [data0, data1, data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = np.vstack(selected_data)\n",
    "labels = np.concatenate([i * np.ones(selected_data[i].shape[0])\n",
    "                         for i in range(len(selected_data))]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.plot(synthetic_data[:, 0], synthetic_data[:, 1], \n",
    "        '.', markersize=10, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealDataGenerator(Dataset):\n",
    "    def __init__(self, numpy_data, maxim=None, minim=None):\n",
    "        super().__init__()\n",
    "        if minim is not None:\n",
    "            numpy_data = numpy_data - minim\n",
    "        if maxim is not None:\n",
    "            numpy_data = numpy_data / (maxim - minim)\n",
    "        self.data = numpy_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx, :]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max = synthetic_data.max(axis=0, keepdims=1)\n",
    "data_min = synthetic_data.min(axis=0, keepdims=1)\n",
    "synthetic_data_normalised = (synthetic_data - data_min) / (data_max - data_min)\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(RealDataGenerator(synthetic_data,\n",
    "                                          data_max,\n",
    "                                          data_min),\n",
    "                        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the data coming from the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    batch_numpy = batch.data\n",
    "#     print(batch_numpy.shape)\n",
    "    ax.plot(batch_numpy[:, 0], batch_numpy[:, 1], \n",
    "            '.', markersize=10, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, latent_dim, dataloader,\n",
    "              optimizer_G, optimizer_D, device, n_epochs,\n",
    "              synthetic_data_normalised):\n",
    "    for epoch in range(n_epochs + 1):\n",
    "#         print(f\"\\n epoch {epoch}:\")\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            bs = batch.shape[0]\n",
    "            real_data = batch.to(device)\n",
    "            real_data_labels = torch.ones(bs, 1).to(device)\n",
    "\n",
    "            generator.to(device)\n",
    "            discriminator.to(device)\n",
    "\n",
    "            # ------------------------------------\n",
    "            # Training the Discriminator:\n",
    "            # ------------------------------------\n",
    "            # generator.requires_grad_(False)\n",
    "            # discriminator.requires_grad_(True)\n",
    "\n",
    "            # generate fake data:\n",
    "            input_noise = torch.rand(bs, latent_dim).to(device)\n",
    "            fake_data = generator(input_noise)\n",
    "            fake_data_true_labels = torch.zeros(bs, 1).to(device)\n",
    "\n",
    "            # Combine reak and fake data into one training batch\n",
    "            data_combined = torch.cat([real_data.double(),\n",
    "                                       fake_data.double()], dim=0)\n",
    "            labels_combined = torch.cat([real_data_labels, fake_data_true_labels], dim=0)\n",
    "\n",
    "            # Shuffle the real and fake data\n",
    "            perm = torch.randperm(bs * 2)\n",
    "            labels_combined = labels_combined[perm, :]\n",
    "            data_combined = data_combined[perm, :]\n",
    "\n",
    "            # Check the discriminator output and update its parameters\n",
    "            optimizer_D.zero_grad()\n",
    "            discriminator_output = discriminator(data_combined.float())\n",
    "            loss = F.cross_entropy(discriminator_output, labels_combined.long().view(-1))\n",
    "            loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # ------------------------------------\n",
    "            # Training the Generator:\n",
    "            # ------------------------------------\n",
    "            # G.requires_grad_(True)\n",
    "            # D.requires_grad_(False)\n",
    "\n",
    "            input_noise = torch.rand(bs, latent_dim).to(device)\n",
    "            fake_data = generator(input_noise)\n",
    "            fake_data_fake_labels = torch.ones(bs, 1).to(device)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            discriminator_output = discriminator(fake_data.float())\n",
    "            loss = F.cross_entropy(discriminator_output, fake_data_fake_labels.long().view(-1))\n",
    "            loss.backward()\n",
    "            optimizer_G.step()\n",
    "        if epoch % 50 == 0:\n",
    "            input_noise = torch.rand(synthetic_data_normalised.shape[0],\n",
    "                                     latent_dim).to(device)\n",
    "            fake_data = generator(input_noise)\n",
    "            batch_numpy = fake_data.cpu().data\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].plot(synthetic_data_normalised[:, 0],\n",
    "                       synthetic_data_normalised[:, 1],\n",
    "                       '.', markersize=3, marker='*', color='b')\n",
    "            ax[0].set_title('Real Data')\n",
    "            ax[1].plot(batch_numpy[:, 0], batch_numpy[:, 1],\n",
    "                       '.', markersize=3, marker='s', color='k')\n",
    "            ax[1].set_title(f'Generated data at epoch {epoch}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "n_epochs = 500\n",
    "latent_dim = 5\n",
    "output_dim = synthetic_data.shape[1]\n",
    "hidden_sizes_gen = (100, 50, 50)\n",
    "hidden_sizes_dis = (100, 50, 50)\n",
    "dropout_p = 0.1\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Instantiate the Generator and Discriminator\n",
    "generator = Generator(latent_dim, output_dim, hidden_sizes_gen)\n",
    "discriminator = Discriminator(output_dim, hidden_sizes_dis, dropout_p)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "train_gan(generator, discriminator, latent_dim, dataloader,\n",
    "          optimizer_G, optimizer_D, device, n_epochs,\n",
    "          synthetic_data_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
