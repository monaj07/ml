{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a25b9a8",
   "metadata": {},
   "source": [
    "# Worker/Helmet/Vest Detection using Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb16b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyyaml==5.1\n",
    "# !pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Install detectron2 that matches the above pytorch version\n",
    "# !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n",
    "# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7979a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pytorch installation: \n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "# assert torch.__version__.startswith(\"1.9\")   # please manually install torch 1.9 if Colab changes its default version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption: matplotlib, numpy, opencv are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1364)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60626a",
   "metadata": {},
   "source": [
    "# Train on a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3bb6a5",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99a041",
   "metadata": {},
   "source": [
    "https://github.com/ciber-lab/pictor-ppe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89097f57",
   "metadata": {},
   "source": [
    "#### Direct link to the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32682738",
   "metadata": {},
   "source": [
    "https://drive.google.com/drive/folders/1M8nzvcnAsEXwz81x18X_mGeqPztZBIvO?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a77af8",
   "metadata": {},
   "source": [
    "#### Unzip the downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip Images-20210810T232206Z-001.zip -d worker_helmet_vest_dataset\n",
    "# !unzip Labels-20210810T234322Z-001.zip -d worker_helmet_vest_dataset/Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c88ed9",
   "metadata": {},
   "source": [
    "#### An example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('worker_helmet_vest_dataset/Images/image_from_china(4182).jpg')\n",
    "img = np.array(img)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e41c7e",
   "metadata": {},
   "source": [
    "### Read the annotations for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('worker_helmet_vest_dataset/Labels/pictor_ppe_crowdsourced_approach-02_train.txt', 'r') as f:\n",
    "    train_set = f.readlines()\n",
    "with open('worker_helmet_vest_dataset/Labels/pictor_ppe_crowdsourced_approach-02_valid.txt', 'r') as f:\n",
    "    val_set = f.readlines()\n",
    "with open('worker_helmet_vest_dataset/Labels/pictor_ppe_crowdsourced_approach-02_test.txt', 'r') as f:\n",
    "    test_set = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b51276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train_set = [re.sub('\\t', ' ', line.strip()) for line in train_set]\n",
    "val_set = [re.sub('\\t', ' ', line.strip()) for line in val_set]\n",
    "test_set = [re.sub('\\t', ' ', line.strip()) for line in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = {}\n",
    "dataset_split['train'] = {\n",
    "    re.findall(r'(image_from_china\\(\\d+\\).jpg).*', line)[0]: \n",
    "        [\n",
    "            [int(item) for item in instance.split(',')] \n",
    "                for instance in re.findall(r'image_from_china\\(\\d+\\).jpg (.*)', line)[0].split()\n",
    "        ] \n",
    "    for line in train_set\n",
    "}\n",
    "#\n",
    "dataset_split['val'] = {\n",
    "    re.findall(r'(image_from_china\\(\\d+\\).jpg).*', line)[0]: \n",
    "        [\n",
    "            [int(item) for item in instance.split(',')] \n",
    "                for instance in re.findall(r'image_from_china\\(\\d+\\).jpg (.*)', line)[0].split()\n",
    "        ] \n",
    "    for line in val_set\n",
    "}\n",
    "#\n",
    "dataset_split['test'] = {\n",
    "    re.findall(r'(image_from_china\\(\\d+\\).jpg).*', line)[0]: \n",
    "        [\n",
    "            [int(item) for item in instance.split(',')] \n",
    "                for instance in re.findall(r'image_from_china\\(\\d+\\).jpg (.*)', line)[0].split()\n",
    "        ] \n",
    "    for line in test_set\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e828ba",
   "metadata": {},
   "source": [
    "### Register train/val/test datasets (converting arbitrary dataset formats to COCO format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1634351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
    "# from detectron2.data.datasets import register_coco_instances\n",
    "# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path/to/image/dir\")\n",
    "# register_coco_instances(\"my_dataset_val\", {}, \"json_annotation_val.json\", \"path/to/image/dir\")\n",
    "\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_worker_dataset_dicts(split):\n",
    "    dataset_dicts = []\n",
    "    \n",
    "    for filename, image_instances in tqdm.tqdm(dataset_split[split].items(), total=len(dataset_split[split]), desc=f'{split}_data loading'):\n",
    "        record = {}\n",
    "        \n",
    "        full_filename = os.path.join('worker_helmet_vest_dataset', 'Images', filename)\n",
    "        img = Image.open(full_filename)\n",
    "        img = np.array(img)\n",
    "        height, width = img.shape[:2]\n",
    "        \n",
    "        record[\"file_name\"] = full_filename\n",
    "        record[\"image_id\"] = 0\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "      \n",
    "        objs = []\n",
    "        for instance in image_instances:\n",
    "            px = [instance[0], instance[2]]\n",
    "            py = [instance[1], instance[3]]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "#                 \"segmentation\": [poly],\n",
    "                \"category_id\": instance[-1], \n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "DatasetCatalog.clear()\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    DatasetCatalog.register(\"worker_dataset_\" + split, lambda split=split: get_worker_dataset_dicts(split))\n",
    "    MetadataCatalog.get(\"worker_dataset_\" + split).set(thing_classes=[\"worker\", \"worker_helmet\", \"worker_vest\", \"worker_helmet_vest\"])\n",
    "worker_dataset_train_metadata = MetadataCatalog.get(\"worker_dataset_train\")\n",
    "worker_dataset_val_metadata = MetadataCatalog.get(\"worker_dataset_val\")\n",
    "worker_dataset_test_metadata = MetadataCatalog.get(\"worker_dataset_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f667dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = get_worker_dataset_dicts('train')\n",
    "worker_dataset_metadata = worker_dataset_train_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df94477",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = Image.open(d['file_name'])\n",
    "    img = np.array(img)\n",
    "    visualizer = Visualizer(img, metadata=worker_dataset_metadata, scale=1)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    print(out.get_image().shape)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n",
    "    imgPIL = Image.fromarray(out.get_image())\n",
    "    imgPIL.save(f\"sample_gt_{os.path.splitext(os.path.basename(d['file_name']))[0]}.jpg\")\n",
    "    ax.imshow(out.get_image())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f145b7b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dbb95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
    "\n",
    "class CocoTrainer(DefaultTrainer):\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            os.makedirs(\"eval_dir\", exist_ok=True)\n",
    "            output_folder = \"eval_dir\"\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29434de2",
   "metadata": {},
   "source": [
    "#### Instantiate a Faster-RCNN config object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"worker_dataset_train\",)\n",
    "cfg.DATASETS.VAL = (\"worker_dataset_val\",)\n",
    "cfg.DATASETS.TEST = (\"worker_dataset_test\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  \n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 5001    \n",
    "cfg.TEST.EVAL_PERIOD = 500\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4e76f",
   "metadata": {},
   "source": [
    "#### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3487d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = CocoTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0195d6e",
   "metadata": {},
   "source": [
    "## Inference/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the COCO Evaluator to use the COCO Metrics\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "# Call the COCO Evaluator function and pass the Validation Dataset\n",
    "evaluator = COCOEvaluator(\"worker_dataset_val\", cfg, False, output_dir=\"output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"worker_dataset_val\")\n",
    "\n",
    "# Use the created predicted model in the previous step\n",
    "inference_on_dataset(predictor.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b14543b",
   "metadata": {},
   "source": [
    "#### Sample Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02527189",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = get_worker_dataset_dicts('test')\n",
    "worker_dataset_metadata = worker_dataset_test_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deedb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = Image.open(d[\"file_name\"])\n",
    "    img = np.array(img)\n",
    "    outputs = predictor(img)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    print(outputs[\"instances\"])\n",
    "    v = Visualizer(img,\n",
    "                   metadata=worker_dataset_metadata, \n",
    "                   scale=1, \n",
    "                   instance_mode=ColorMode.IMAGE \n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    imgPIL = Image.fromarray(out.get_image())\n",
    "    imgPIL.save(f\"sample_pred_fasterrcnn_{os.path.splitext(os.path.basename(d['file_name']))[0]}.jpg\")\n",
    "    plt.imshow(out.get_image())\n",
    "    plt.show()\n",
    "    # print(outputs[\"instances\"].to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d88c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "old_detectron",
   "language": "python",
   "name": "old_detectron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
