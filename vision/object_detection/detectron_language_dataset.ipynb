{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2502b5df",
   "metadata": {},
   "source": [
    "# Multi-Class Object Detection using Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install dependencies: (use cu101 because colab has CUDA 10.1)\n",
    "# !pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
    "# !pip install cython pyyaml==5.1\n",
    "# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "# import torch, torchvision\n",
    "# print(torch.__version__, torch.cuda.is_available())\n",
    "# !gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pytorch installation: \n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "# assert torch.__version__.startswith(\"1.9\")   # please manually install torch 1.9 if Colab changes its default version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf8e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption: matplotlib, numpy, opencv are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4ff32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import glob\n",
    "import os, json, cv2, random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee25ea0",
   "metadata": {},
   "source": [
    "### Text Language Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://miro.medium.com/max/2000/1*2c5DytyDMLlYma3nqYCbLg.png\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd7911b",
   "metadata": {},
   "source": [
    "Here is the link to the dataset:\n",
    "\n",
    "https://github.com/aakarsh7599/Text-Detection-using-Detectron2/blob/master/Custom_Dataset_Format_to_COCO_Format_Conversion.ipynb\n",
    "\n",
    "(https://drive.google.com/file/d/1gZW8WiQz5UYPXo97nmcP7AI8dHH1yqPM/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44257e73",
   "metadata": {},
   "source": [
    "### Raw annotations in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a5379",
   "metadata": {},
   "source": [
    "* This dataset consists of 428 real images in the image folder. Annotation corresponding to image presents in Annotation folder. Out of which 401 images are used for training and the remaining 27 images are used for validation.\n",
    "* The Annotation for the image has the same name that of the image just with the difference of extension. For example, if the image name is \"1.jpg\" then the corresponding annotation will be \"1.txt\".\n",
    "* The format for the storage of the annotation file is as described here:\n",
    " *  The no. of the lines in annotation text file denotes no of bounding box present in that image.\n",
    " *  A single line represents a single bounding box. format is as follow x1, x2, x3, x4, y1, y2, y3, y4, Language. Where (x1,y1) is the top left, (x2,y2) is top right, (x3,y3) bottom right, (x4,y4) bottom left.\n",
    " *  The order of point is in the clockwise order starting from the top-left points."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce6235c8",
   "metadata": {},
   "source": [
    "52,146,147,53,343,340,397,400,HINDI\n",
    "157,246,250,162,338,339,392,397,HINDI\n",
    "270,345,348,279,337,335,389,391,HINDI\n",
    "363,578,585,364,338,330,380,388,ENGLISH\n",
    "125,265,267,131,423,420,472,471,ENGLISH\n",
    "272,468,470,282,423,418,464,470,HINDI\n",
    "477,519,528,478,406,403,465,466,HINDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aed7ea",
   "metadata": {},
   "source": [
    "We must change this format to **COCO** format to make it compatible with detectron2, i.e. this form:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c1d3e",
   "metadata": {},
   "source": [
    "![](json_coco_annotation_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b3b1b",
   "metadata": {},
   "source": [
    "### Convert the raw annotations to COCO-friendly annotation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict= {\"HINDI\":\"0\", \"ENGLISH\":\"1\", \"OTHER\":\"2\"}\n",
    "header_list = [\"x1\", \"x2\", \"x3\", \"x4\", \"y1\", \"y2\", \"y3\", \"y4\", \"category_id\"]\n",
    "\n",
    "for split in [\"Train\", \"Val\"]:\n",
    "    # Instantiate the json_records\n",
    "    json_records = pd.DataFrame(columns=[\"file_name\", \"height\", \"width\", \"annotations\"])\n",
    "    json_records[\"annotations\"] = json_records[\"annotations\"].astype('object')\n",
    "    \n",
    "    # list the annotation text files (one file for each image)\n",
    "    file_list = natsorted(glob.glob(f'data/Text Detection Dataset/{split}/Annotations/*.txt'))\n",
    "    \n",
    "    # Iterate through the above list, file by file\n",
    "    for k, filepath in tqdm(enumerate(file_list), desc=split, total=len(file_list)):\n",
    "        # Read all objects (lines) in this file at once into a dataframe\n",
    "        df = pd.read_csv(filepath, header=None, index_col=False, names=header_list)\n",
    "        \n",
    "        # Convert the dataframe format to a COCO compatible format (BoxMode.XYWH_ABS)\n",
    "        df[\"height\"] = abs(df[\"y1\"] - df[\"y3\"])\n",
    "        df[\"width\"] = abs(df[\"x1\"] - df[\"x3\"])\n",
    "        df = df[[\"x1\", \"y1\", \"width\", \"height\", \"category_id\"]]\n",
    "\n",
    "        # For each row/object, pack (\"x1\", \"y1\", \"width\", \"height\") into a `list` variable \n",
    "        # and put it into a new column named \"bbox\"\n",
    "        df[\"bbox\"] = df.iloc[:, 0:4].values.tolist()\n",
    "        \n",
    "        df[\"bbox_mode\"] = 1  # BoxMode.XYWH_ABS\n",
    "        \n",
    "        # Replace the string labels with integer labels\n",
    "        df = df.replace({\"category_id\": cat_dict})\n",
    "        \n",
    "#         print(df.category_id, os.path.basename(filepath))\n",
    "        \n",
    "        \n",
    "        # Only keep these three columns required by COCO\n",
    "        df = df[[\"bbox\", \"bbox_mode\", \"category_id\"]]\n",
    "        \n",
    "        # Serialise this dataframe of object bounding boxes into a list\n",
    "        annotations = df.T.to_dict().values()\n",
    "        bboxes = []\n",
    "        for j in annotations:\n",
    "            bboxes.append(j)\n",
    "            \n",
    "        # Define a record for the current image/filepath, and add the required information\n",
    "        # Required information: file_name, annotations, img_height, img_width\n",
    "        record = pd.DataFrame(columns=[\"file_name\", \"height\", \"width\", \"annotations\"])\n",
    "        record[\"annotations\"] = record[\"annotations\"].astype('object')\n",
    "        file_name = os.path.basename(filepath)\n",
    "        file_name = os.path.splitext(file_name)[0] + \".jpeg\"\n",
    "        record.at[0, \"file_name\"] = file_name\n",
    "        record.at[0, \"annotations\"] = bboxes\n",
    "        img = cv2.imread(f'data/Text Detection Dataset/{split}/Images/{file_name}')\n",
    "        record.at[0, \"height\"] = img.shape[0]\n",
    "        record.at[0, \"width\"] = img.shape[1]\n",
    "        json_records = json_records.append(record)\n",
    "        json_records.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    json_records.reset_index(inplace=True)\n",
    "    json_records.rename(columns={\"index\": \"image_id\"}, inplace=True)\n",
    "    json_records.to_json(f\"data/Text Detection Dataset/{split}/coco_records.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = [box['category_id'] for record in json_records['annotations'].tolist() for box in record]\n",
    "pd.Series(all_ids).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52775657",
   "metadata": {},
   "source": [
    "### Register the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A few annotation text files have some mistakes, they have excessive coordinate values in the bbox lines (10 numbers instead of 8 numbers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad19792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_board_dicts(imgdir):\n",
    "    json_file = imgdir + \"/coco_records.json\"  # Fetch the json file\n",
    "    with open(json_file) as f:\n",
    "        dataset_dicts = json.load(f)\n",
    "    for record in dataset_dicts:\n",
    "        file_name = record[\"file_name\"] \n",
    "        record[\"file_name\"] = os.path.join(imgdir, 'Images', file_name)\n",
    "        for bbox in record[\"annotations\"]:\n",
    "            bbox[\"bbox_mode\"] = BoxMode.XYWH_ABS  # Setting the required Box Mode\n",
    "            bbox[\"category_id\"] = int(bbox[\"category_id\"])\n",
    "            if bbox[\"category_id\"] not in (0, 1, 2):\n",
    "                print(\"Annotations have mistakes, they have excessive coordinate values in the bbox lines\")\n",
    "    return dataset_dicts\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "# Registering the Datasets\n",
    "for split in [\"Train\", \"Val\"]:\n",
    "    DatasetCatalog.register(f\"board_language_{split}\", lambda split=split: get_board_dicts(f\"data/Text Detection Dataset/{split}\"))\n",
    "    MetadataCatalog.get(f\"board_language_{split}\").set(thing_classes=[\"HINDI\", \"ENGLISH\", \"OTHER\"])\n",
    "board_metadata = MetadataCatalog.get(f\"board_language_Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f2705",
   "metadata": {},
   "source": [
    "#### Visualise some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = get_board_dicts(f\"data/Text Detection Dataset/Train\")\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=board_metadata, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    plt.imshow(out.get_image())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded95928",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from detectron2.data import detection_utils as utils\n",
    "# import detectron2.data.transforms as T\n",
    "# import copy\n",
    "\n",
    "# def custom_mapper(dataset_dict):\n",
    "#     dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "#     image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "#     transform_list = [\n",
    "#         T.Resize((800,600)),\n",
    "#         T.RandomBrightness(0.8, 1.8),\n",
    "#         T.RandomContrast(0.6, 1.3),\n",
    "#         T.RandomSaturation(0.8, 1.4),\n",
    "#         T.RandomRotation(angle=[90, 90]),\n",
    "#         T.RandomLighting(0.7),\n",
    "#         T.RandomFlip(prob=0.4, horizontal=False, vertical=True),\n",
    "#     ]\n",
    "#     image, transforms = T.apply_transform_gens(transform_list, image)\n",
    "#     dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "#     annos = [\n",
    "#         utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "#         for obj in dataset_dict.pop(\"annotations\")\n",
    "#         if obj.get(\"iscrowd\", 0) == 0\n",
    "#     ]\n",
    "#     instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "#     dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "#     return dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3464736",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
    "\n",
    "class CocoTrainer(DefaultTrainer):\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            os.makedirs(\"coco_eval\", exist_ok=True)\n",
    "            output_folder = \"coco_eval\"\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "    \n",
    "#     # Data Augmentation overloader\n",
    "#     @classmethod\n",
    "#     def build_train_loader(cls, cfg):\n",
    "#         return build_detection_train_loader(cfg, mapper=custom_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa50ed6",
   "metadata": {},
   "source": [
    "#### Set the Detectron Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53addbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "import os\n",
    "\n",
    "cfg = get_cfg()\n",
    "\n",
    "# Get the basic model configuration from the model zoo \n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "# Passing the Train and Validation sets\n",
    "cfg.DATASETS.TRAIN = (\"board_language_Train\",)\n",
    "cfg.DATASETS.TEST = (\"board_language_Val\",)\n",
    "\n",
    "# Number of data loading threads\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "\n",
    "# Let training initialize from model zoo\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  \n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.0125\n",
    "cfg.SOLVER.MAX_ITER = 1500\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256  \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3 \n",
    "cfg.TEST.EVAL_PERIOD = 500\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CocoTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83361092",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c0c01",
   "metadata": {},
   "source": [
    "### Inference on example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "# Use the final weights generated after successful training for inference  \n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "# set the testing threshold for this model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  \n",
    "\n",
    "# Pass the validation dataset\n",
    "cfg.DATASETS.TEST = (\"board_language_Val\", )\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "dataset_dicts = get_board_dicts(\"data/Text Detection Dataset/Val\")\n",
    "for d in random.sample(dataset_dicts, 3):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=board_metadata, \n",
    "                   scale=0.8,\n",
    "                   instance_mode=ColorMode.IMAGE   \n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\")) \n",
    "    plt.imshow(v.get_image())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d82c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the COCO Evaluator to use the COCO Metrics\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "# Call the COCO Evaluator function and pass the Validation Dataset\n",
    "evaluator = COCOEvaluator(\"board_language_Val\", cfg, False, output_dir=\"output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"board_language_Val\")\n",
    "\n",
    "# Use the created predicted model in the previous step\n",
    "inference_on_dataset(predictor.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba12d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ca342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "old_detectron",
   "language": "python",
   "name": "old_detectron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
