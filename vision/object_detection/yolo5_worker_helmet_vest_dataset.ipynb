{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af2f2aa",
   "metadata": {},
   "source": [
    "### This notebook should be executed from the main path of the Yolo5 repository:\n",
    "\n",
    "i.e. from here:\n",
    "https://github.com/ultralytics/yolov5\n",
    "\n",
    "In fact instead of running this notebook, you can just run this one:\n",
    "https://github.com/monaj07/yolov5/blob/b85e87855823f740b10daa12e1e7b09839f353c0/yolo5_finetuning.ipynb,\n",
    "which is the same as the current notebook.\n",
    "The current notebook is here just as a reminder to not forget the above one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626421e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyYAML==5.3.1\n",
    "# !pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62500d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# def overlay_bbox(img, label, box_points):\n",
    "#     H, W = img.shape[:-1]\n",
    "#     p1, p2 = box_points\n",
    "#     x1, y1 = p1['x'] * W, p1['y'] * H\n",
    "#     x2, y2 = p2['x'] * W, p2['y'] * H\n",
    "\n",
    "#     cv2.rectangle(img, \n",
    "#                   (int(x1), int(y1)),\n",
    "#                   (int(x2), int(y2)),\n",
    "#                   color=(0, 255, 0),\n",
    "#                   thickness=2)\n",
    "    \n",
    "#     (label_width, label_height), _ = cv2.getTextSize(\n",
    "#         label, \n",
    "#         fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "#         fontScale=1.75, \n",
    "#         thickness=2)\n",
    "\n",
    "#     cv2.rectangle(img, \n",
    "#                   (int(x1), int(y1)),\n",
    "#                   (int(x1 + label_width), int(y1 + label_height)),\n",
    "#                   color=(0, 255, 0),\n",
    "#                   thickness=cv2.FILLED)\n",
    "    \n",
    "#     cv2.putText(\n",
    "#         img,\n",
    "#         label,\n",
    "#         org=(int(x1), int(y1 + label_height)),\n",
    "#         fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "#         fontScale=1.75,\n",
    "#         color=(255, 255, 255),\n",
    "#         thickness=2\n",
    "#     )\n",
    "\n",
    "#     return img\n",
    "\n",
    "# img_bbox = overlay_bbox(img, \n",
    "#                         train_data[0]['annotation'][0]['label'][0], \n",
    "#                         train_data[0]['annotation'][0]['points'])\n",
    "# plt.imshow(img_bbox); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a8f77",
   "metadata": {},
   "source": [
    "# Train on a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813482d5",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6192b35",
   "metadata": {},
   "source": [
    "https://github.com/ciber-lab/pictor-ppe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c55220",
   "metadata": {},
   "source": [
    "#### Direct link to the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3e790",
   "metadata": {},
   "source": [
    "https://drive.google.com/drive/folders/1M8nzvcnAsEXwz81x18X_mGeqPztZBIvO?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0f2ee",
   "metadata": {},
   "source": [
    "#### Unzip the downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip Images-20210810T232206Z-001.zip -d worker_helmet_vest_dataset\n",
    "# !unzip Labels-20210810T234322Z-001.zip -d worker_helmet_vest_dataset/Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1dae7e",
   "metadata": {},
   "source": [
    "#### An example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('../worker_helmet_vest_dataset/Images/image_from_china(4182).jpg')\n",
    "img = np.array(img)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8b137",
   "metadata": {},
   "source": [
    "### Convert the raw dataset to a Yolo5 friendly dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cd851",
   "metadata": {},
   "source": [
    "Read this section carefully:\n",
    "https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#2-create-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d05830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import tqdm\n",
    "\n",
    "def create_dataset(path='../worker_helmet_vest_dataset'):\n",
    "    \"\"\"\n",
    "    YOLO v5 requires the dataset to be in the darknet format. \n",
    "    Here's an outline of what it looks like:\n",
    "\n",
    "    One txt with labels file per image\n",
    "    One row per object\n",
    "    Each row: class_index bbox_x_center bbox_y_center bbox_width bbox_height\n",
    "    Box coordinates must be normalized between 0 and 1\n",
    "    \"\"\"\n",
    "    with open(os.path.join(path, 'Labels', 'pictor_ppe_crowdsourced_approach-02_train.txt'), 'r') as f:\n",
    "        train_set = f.readlines()\n",
    "    with open(os.path.join(path, 'Labels', 'pictor_ppe_crowdsourced_approach-02_valid.txt'), 'r') as f:\n",
    "        val_set = f.readlines()\n",
    "    with open(os.path.join(path, 'Labels', 'pictor_ppe_crowdsourced_approach-02_test.txt'), 'r') as f:\n",
    "        test_set = f.readlines()\n",
    "        \n",
    "    train_set = [re.sub('\\t', ' ', line.strip()) for line in train_set]\n",
    "    val_set = [re.sub('\\t', ' ', line.strip()) for line in val_set]\n",
    "    test_set = [re.sub('\\t', ' ', line.strip()) for line in test_set]\n",
    "    \n",
    "    dataset_split = {}\n",
    "    dataset_split['train'] = {\n",
    "        re.findall(r'(image_from_china\\(\\d+\\).jpg).*', line)[0]: \n",
    "            [\n",
    "                [int(item) for item in instance.split(',')] \n",
    "                    for instance in re.findall(r'image_from_china\\(\\d+\\).jpg (.*)', line)[0].split()\n",
    "            ] \n",
    "        for line in train_set\n",
    "    }\n",
    "    #\n",
    "    dataset_split['val'] = {\n",
    "        re.findall(r'(image_from_china\\(\\d+\\).jpg).*', line)[0]: \n",
    "            [\n",
    "                [int(item) for item in instance.split(',')] \n",
    "                    for instance in re.findall(r'image_from_china\\(\\d+\\).jpg (.*)', line)[0].split()\n",
    "            ] \n",
    "        for line in val_set\n",
    "    }\n",
    "    #\n",
    "    dataset_split['test'] = {\n",
    "        re.findall(r'(image_from_china\\(\\d+\\).jpg).*', line)[0]: \n",
    "            [\n",
    "                [int(item) for item in instance.split(',')] \n",
    "                    for instance in re.findall(r'image_from_china\\(\\d+\\).jpg (.*)', line)[0].split()\n",
    "            ] \n",
    "        for line in test_set\n",
    "    }    \n",
    "    \n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        image_path = os.path.join(path, 'images', split)\n",
    "        os.makedirs(image_path, exist_ok=True)\n",
    "        label_path = os.path.join(path, 'labels', split)\n",
    "        os.makedirs(label_path, exist_ok=True)\n",
    "\n",
    "        for filename, image_instances in tqdm.tqdm(dataset_split[split].items(), total=len(dataset_split[split]), desc=f'{split}_data loading'):\n",
    "            record = {}\n",
    "\n",
    "            full_filename = os.path.join(path, 'Images', filename)\n",
    "            full_filename_dest = os.path.join(image_path, filename)\n",
    "            \n",
    "            img = Image.open(full_filename)\n",
    "            img = np.array(img)\n",
    "            height, width = img.shape[:2]\n",
    "            shutil.copyfile(full_filename, full_filename_dest)\n",
    "\n",
    "            with open(os.path.join(label_path, '{}.txt'.format(re.findall(r\"(.+)\\..+\", filename)[0])), 'w') as f:\n",
    "                for instance in image_instances:                  \n",
    "                    \n",
    "                    py = [instance[1]/height, instance[3]/height]\n",
    "                    px = [instance[0]/width, instance[2]/width]\n",
    "                                        \n",
    "                    bbox_width = px[1] - px[0]\n",
    "                    bbox_height = py[1] - py[0]\n",
    "                    f.write(\n",
    "                      f\"{instance[-1]} {px[0] + bbox_width / 2} {py[0] + bbox_height / 2} {bbox_width} {bbox_height}\\n\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82454ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(path='../worker_helmet_vest_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea713a",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "* img 320 - resize the images to 320x320 pixels (Larger is better, e.g. 640)\n",
    "* batch 4 - 4 images per batch\n",
    "* epochs 30 - train for 30 epochs\n",
    "* data ./data/worker_helmet_vest_dataset.yaml - path to dataset config *(number of classes, path to the dataset, class labels)*\n",
    "* cfg ./models/yolov5x_**4class**.yaml - model config\n",
    "* weights yolov5x.pt - use pre-trained weights from the YOLOv5x model\n",
    "* name yolov5x_clothing - name of our model\n",
    "* cache - cache dataset images for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74587ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 640 --batch 4 --epochs 30 \\\n",
    "    --data data/worker_helmet_vest_dataset.yaml \\\n",
    "    --cfg models/yolov5x_4class.yaml \\\n",
    "    --weights yolov5x.pt \\\n",
    "    --name yolov5x_worker_helmet_vest_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344f350",
   "metadata": {},
   "source": [
    "#### Plot the training metric curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44f19c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plots import plot_results\n",
    "\n",
    "plot_results('./runs/train/yolov5x_worker_helmet_vest_dataset/results.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71b44d",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec989ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights runs/train/yolov5x_worker_helmet_vest_dataset/weights/best.pt \\\n",
    "  --img 640 --conf 0.4 --source ../worker_helmet_vest_dataset/images/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "def load_image(path, resize=True):\n",
    "    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (256, 256), interpolation = cv2.INTER_AREA)\n",
    "    return img\n",
    "\n",
    "def show_grid(image_paths):\n",
    "    images = [load_image(img) for img in image_paths]\n",
    "    images = torch.as_tensor(images)\n",
    "    images = images.permute(0, 3, 1, 2)\n",
    "    print(images.shape)\n",
    "    grid_img = torchvision.utils.make_grid(images, nrow=3)\n",
    "    plt.figure(figsize=(24, 24))\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c71788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "img_paths = list(glob(\"runs/detect/exp/*.jpg\"))[:9]\n",
    "show_grid(img_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
