{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a set of data points $X_{1:N}:\\{x_1, x_2, \\dots, x_N\\}$ sampled from a distribution $p(x)$, and we want to acquire more samples from $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could potentially use [*Inverse Sampling*](https://en.wikipedia.org/wiki/Inverse_transform_sampling) method, where we first sample $\\widetilde{u}$ from a uniform distribution $U[0, 1]$ and pass it to the inverse of the *CDF* of $p(x)$ to generate our new sample $\\widetilde{x}$:\n",
    "\n",
    "$\\tilde{u} \\sim U[0, 1] \\, , \\, \\tilde{x} = CDF^{-1}_{p(x)}(\\tilde{u})$\n",
    "\n",
    "This method works only when $p(x)$ is quite starightforward and computing its inverse CDF is not difficult.\n",
    "This however is not the case most of the times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejection Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case $p(x)$ is quite convoluted such that inverse sampling is not feasible, one could possibly use [*Rejection Sampling*](https://en.wikipedia.org/wiki/Rejection_sampling) (a.k.a. accept/reject sampling). In this approach, a proposal distribution $q(x)$ with analytically closed form. Then an envelope function $e(x) = \\alpha q(x)$ is created such that $e(x)=\\alpha q(x) > p(x) \\,\\,\\, \\forall x, \\,\\,\\, \\alpha \\ge 1$.\n",
    "Then we follow this procedure:\n",
    "\n",
    "At each iteration:\n",
    "\n",
    "1) take a sample $\\widetilde{x}$ from $q(x)$ and $\\widetilde{u}$ from $U[0, 1]$. (*Remember that $q(x)$ must have been chosen to be an analytically simple distribution function, so taking a sample from it should be easy, e.g. using Inverse Sampling*).\n",
    "\n",
    "2) If $\\widetilde{u} < \\frac{p(x)}{e(x)}$, accept the sample $\\widetilde{x}$, otherwise reject it.\n",
    "\n",
    "3) Go back to step-1 to try another sample.\n",
    "![Rejection Sampling](files/rejectionsamplingcriterion.png)\n",
    "Figure taken from: https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/\n",
    "\n",
    "If we are lucky/clever and our proposal $e(x)$ is very close to the actual distribution $p(x)$, then we almost always get $\\frac{p(x)}{e(x)} \\approx 1$, hence most of the samples are accepted by our criteria, and sampling will be done faster.\n",
    "\n",
    "Another example, where the underlying distribution is very intricate and we provide a simple proposal and scale it such that it envelopes the entire of our actual distribution and start the accept/reject sampling process:\n",
    "![caption](files/RejectionSampling2.jpg)\n",
    "Figure taken from: http://ct-cps.blogspot.com/2011/04/rejection-sampling-algorithm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Sampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high-dimensional data, Rejection sampling is super inefficient as a lot of samples are rejected due to the curse of dimensionality. Therefore, people tend to use other methods such as Importance Sampling, MCMC, Gibbs sampling, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown distribution $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take make our problem a bit more complicated by assuming that we don't know the underlying distribution $p(x)$ and all we know about it is a set of already withdrawn samples $X_{1:N}:\\{x_1, x_2, \\dots, x_N\\}$ .(e.g. a number of paintings drawn by an artist from 18th century).\n",
    "\n",
    "Previously when we knew $p(x)$, we started from a uniformly sampled variable $\\widetilde{u}$ and used either Inverse sampling, Rejection sampling, MCMC, or other methods to get from $\\widetilde{u}$ to $\\widetilde{x}$. (For instance, $\\widetilde{u} \\xrightarrow{CDF^{-1}_{p(x)}} \\widetilde{x}$).\n",
    "In other words, our knowledge about $p(x)$ was used in order to find the mapping. \n",
    "\n",
    "However, when $p(x)$ is unknown, this information is missing and we have to fill the black-box between $\\widetilde{u}$ and $\\widetilde{x}$ with some other technique. The main idea behind GANs is to use a **Generator** neural network to model that mapping $\\widetilde{u} \\xrightarrow{G(\\theta)} \\widetilde{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Generator $G(\\theta)$ (by training a discriminator $D(\\phi)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator network needs to learn how to map the input uniform sample to the real samples. One way would be to define a reconstruction loss function for it: $L = \\|\\widetilde{x} - x\\|^2$. This direct comparison of the distribution samples and generated samples is not very straightforward and also does not produce good new samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we assess our generated samples in another way, by introducing a neural network called **discriminator** $D(\\phi)$. Discriminator is trained to differentiate between the fake/generated samples and the real samples. In other words, at each iteraion of GAN training, the samples generated by $G(\\widetilde{u}; \\theta)$ are examined by $D(\\widetilde{x}; \\phi)$. The discriminator is trained to properly separate fake data from real data (two-class supervised learning). Moreover, Generator is trained such that its generated samples get a positive prodeiction from the discriminator. As we continue with this training process, Discriminator gets better and better at distinguishing positive and negative classes, and Generator gets better and better at generaing real-looking samples. This battle continues until discriminator cannot discriminate anymore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically speaking, the objective of discriminator is a binary cross-entropy over positive labels (real data: $x$) and negative labels (fake data: $G(\\widetilde{u}; \\theta)$):\n",
    "\n",
    "\\begin{equation} \n",
    "\\max_D\\Bigg\\{E_{x\\sim p(x)}\\Big[\\log\\big(D(x; \\phi\\big)\\Big] \\, + \\, E_{\\widetilde{u}\\sim U[0, 1]}\\Big[\\log\\big(1 - D\\big(G(\\widetilde{u}; \\theta); \\phi\\big)\\big)\\Big]\\Bigg\\}\n",
    "\\tag{I}\n",
    "\\end{equation} \n",
    "\n",
    "In this objective function, $\\theta$ is fixed and we only update the parameters of $D$ (i.e. $\\phi$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time the objective of generator is to maximise the probability of getting a positive prediction label from the discriminator, or in other words, minimizing the value of $1 - D\\big(G(\\widetilde{u}; \\theta); \\phi\\big)$ (while discriminator parameters, $\\phi$, are fixed:\n",
    "\\begin{equation} \n",
    "\\min_G\\Big\\{E_{\\widetilde{u}\\sim U[0, 1]}\\big[\\log(1 - D\\big(G(\\widetilde{u}; \\theta); \\phi\\big)\\big]\\Big\\}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this objective function more stable (c.f. page 116 in your hand-written notebook), it is re-written as:\n",
    "\\begin{equation} \n",
    "\\max_G\\Big\\{E_{\\widetilde{u}\\sim U[0, 1]}\\big[\\log( D\\big(G(\\widetilde{u}; \\theta); \\phi\\big)\\big]\\Big\\}\n",
    "\\tag{II}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the training process will alternately perform gradient ascent (or gradient descent on negative of these two log-likelihoods (equations **I** and **II**)) to update the networks and thereby, generate real-looking samples as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=5, output_dim=2, hidden_sizes=(10, 5)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(latent_dim, hidden_sizes[0]),\n",
    "                       nn.BatchNorm1d(hidden_sizes[0]),\n",
    "                       nn.ReLU6(inplace=True)])\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.BatchNorm1d(hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU6(inplace=True))\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_dim))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        out = torch.sigmoid(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_sizes=(10, 5), dropout_p=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_dim, hidden_sizes[0]),\n",
    "                       nn.LeakyReLU(0.2, inplace=True),\n",
    "                       nn.Dropout(dropout_p)])\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Dropout(dropout_p))\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 2))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underlying distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [1000, 1000, 1000]\n",
    "mu0 = np.array([-13, 15])\n",
    "cov0 = np.array([[6, 5], [5, 6]])\n",
    "data0 = np.random.multivariate_normal(mu0, cov0, size=sizes[0])\n",
    "mu1 = np.array([0, -3])\n",
    "cov1 = np.array([[20, 0], [0, 1]])\n",
    "data1 = np.random.multivariate_normal(mu1, cov1, size=sizes[1])\n",
    "mu2 = np.array([10, 13])\n",
    "cov2 = np.array([[3, 0], [0, 3]])\n",
    "data2 = np.random.multivariate_normal(mu2, cov2, size=sizes[2])\n",
    "\n",
    "selected_data = [data0, data1, data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = np.vstack(selected_data)\n",
    "labels = np.concatenate([i * np.ones(selected_data[i].shape[0])\n",
    "                         for i in range(len(selected_data))]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.plot(synthetic_data[:, 0], synthetic_data[:, 1], \n",
    "        '.', markersize=10, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealDataGenerator(Dataset):\n",
    "    def __init__(self, numpy_data, maxim=None, minim=None):\n",
    "        super().__init__()\n",
    "        if minim is not None:\n",
    "            numpy_data = numpy_data - minim\n",
    "        if maxim is not None:\n",
    "            numpy_data = numpy_data / (maxim - minim)\n",
    "        self.data = numpy_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx, :]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max = synthetic_data.max(axis=0, keepdims=1)\n",
    "data_min = synthetic_data.min(axis=0, keepdims=1)\n",
    "synthetic_data_normalised = (synthetic_data - data_min) / (data_max - data_min)\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(RealDataGenerator(synthetic_data,\n",
    "                                          data_max,\n",
    "                                          data_min),\n",
    "                        batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the data coming from the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    batch_numpy = batch.data\n",
    "#     print(batch_numpy.shape)\n",
    "    ax.plot(batch_numpy[:, 0], batch_numpy[:, 1], \n",
    "            '.', markersize=10, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, the discriminator is trained first using two batches of data from real samples and fake samples.\n",
    "Then the generator is trained by setting its target to be positive and updating its parameters to make the predictions of its generated samples to be close to this fake target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, latent_dim, dataloader,\n",
    "              optimizer_G, optimizer_D, device, n_epochs,\n",
    "              synthetic_data_normalised):\n",
    "    for epoch in range(n_epochs + 1):\n",
    "#         print(f\"\\n epoch {epoch}:\")\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            bs = batch.shape[0]\n",
    "            real_data = batch.to(device)\n",
    "            real_data_labels = torch.ones(bs, 1).to(device)\n",
    "\n",
    "            generator.to(device)\n",
    "            discriminator.to(device)\n",
    "\n",
    "            # ------------------------------------\n",
    "            # Training the Discriminator:\n",
    "            # ------------------------------------\n",
    "            # generator.requires_grad_(False)\n",
    "            # discriminator.requires_grad_(True)\n",
    "\n",
    "            # generate fake data:\n",
    "            input_noise = torch.rand(bs, latent_dim).to(device)\n",
    "            fake_data = generator(input_noise)\n",
    "            fake_data_true_labels = torch.zeros(bs, 1).to(device)\n",
    "\n",
    "            # Combine reak and fake data into one training batch\n",
    "            data_combined = torch.cat([real_data.double(),\n",
    "                                       fake_data.double()], dim=0)\n",
    "            labels_combined = torch.cat([real_data_labels, fake_data_true_labels], dim=0)\n",
    "\n",
    "            # Shuffle the real and fake data\n",
    "            perm = torch.randperm(bs * 2)\n",
    "            labels_combined = labels_combined[perm, :]\n",
    "            data_combined = data_combined[perm, :]\n",
    "\n",
    "            # Check the discriminator output and update its parameters\n",
    "            optimizer_D.zero_grad()\n",
    "            discriminator_output = discriminator(data_combined.float())\n",
    "            loss = F.cross_entropy(discriminator_output, labels_combined.long().view(-1))\n",
    "            loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # ------------------------------------\n",
    "            # Training the Generator:\n",
    "            # ------------------------------------\n",
    "            # G.requires_grad_(True)\n",
    "            # D.requires_grad_(False)\n",
    "\n",
    "            input_noise = torch.rand(bs, latent_dim).to(device)\n",
    "            fake_data = generator(input_noise)\n",
    "            fake_data_fake_labels = torch.ones(bs, 1).to(device)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            discriminator_output = discriminator(fake_data.float())\n",
    "            loss = F.cross_entropy(discriminator_output, fake_data_fake_labels.long().view(-1))\n",
    "            loss.backward()\n",
    "            optimizer_G.step()\n",
    "        if epoch % 50 == 0:\n",
    "            input_noise = torch.rand(synthetic_data_normalised.shape[0],\n",
    "                                     latent_dim).to(device)\n",
    "            fake_data = generator(input_noise)\n",
    "            batch_numpy = fake_data.cpu().data\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].plot(synthetic_data_normalised[:, 0],\n",
    "                       synthetic_data_normalised[:, 1],\n",
    "                       '.', markersize=3, marker='*', color='b')\n",
    "            ax[0].set_title('Real Data')\n",
    "            ax[1].plot(batch_numpy[:, 0], batch_numpy[:, 1],\n",
    "                       '.', markersize=3, marker='s', color='k')\n",
    "            ax[1].set_title(f'Generated data at epoch {epoch}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "n_epochs = 500\n",
    "latent_dim = 5\n",
    "output_dim = synthetic_data.shape[1]\n",
    "hidden_sizes_gen = (100, 50, 50)\n",
    "hidden_sizes_dis = (100, 50, 50)\n",
    "dropout_p = 0.1\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Instantiate the Generator and Discriminator\n",
    "generator = Generator(latent_dim, output_dim, hidden_sizes_gen)\n",
    "discriminator = Discriminator(output_dim, hidden_sizes_dis, dropout_p)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "train_gan(generator, discriminator, latent_dim, dataloader,\n",
    "          optimizer_G, optimizer_D, device, n_epochs,\n",
    "          synthetic_data_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
