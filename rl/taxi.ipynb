{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the environment üïπÔ∏è\n",
    "- Here we'll create the Taxi-v3 environment üöï. \n",
    "\n",
    "[Environment]\n",
    "\n",
    "Our environment looks like this: \n",
    "- It's a **5x5 grid world**\n",
    "- Our üöï is spawned **randomly** in a square. \n",
    "- The passenger is **spawned randomly in one of the 4 possible locations** (R, B, G, Y) and wishes to go in one of the **4 possibles locations too**.\n",
    "\n",
    "The reward system:\n",
    "- -1 for each timestep\n",
    "- +20 for successfully deliver the passenger\n",
    "- -10 for illegal actions (pickup or putdown the passenger at the outside of the destination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the following description ([reference](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Taxi Problem\n",
    "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
    "    by Tom Dietterich\n",
    "    \n",
    "    Description:\n",
    "    There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "    \n",
    "    Observations:\n",
    "    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
    "    Note that there are 400 states that can actually be reached during an episode. The missing states correspond to situations in which the passenger is at the same location as their destination, as this typically signals the end of an episode.\n",
    "    Four additional states can be observed right after a successful episodes, when both the passenger and the taxi are at the destination.\n",
    "    This gives a total of 404 reachable discrete states.\n",
    "    \n",
    "    Passenger locations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi\n",
    "    \n",
    "    Destinations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    \n",
    "    Actions:\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east\n",
    "    - 3: move west\n",
    "    - 4: pickup passenger\n",
    "    - 5: drop off passenger\n",
    "    \n",
    "    Rewards:\n",
    "    There is a default per-step reward of -1,\n",
    "    except for delivering the passenger, which is +20,\n",
    "    or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "    \n",
    "    Rendering:\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - yellow: empty taxi\n",
    "    - green: full taxi\n",
    "    - other letters (R, G, Y and B): locations for passengers and destinations\n",
    "    \n",
    "    state space is represented by:\n",
    "        (taxi_row, taxi_col, passenger_location, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read a state from the Rendering"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above description and playing a little bit with the renderer, I realised the grid is represented this way:\n",
    "\n",
    "Starting from the top-left, moving row-wise, and ending to the bottom-right, each cell takes 20 different sub-states (zero to 19), where each sub-state is determined by the location of the passenger (one of the 5 possible options) times ($\\times$) the location of the destination (one of the four options). \n",
    "\n",
    "For example, if we see this rendering:\n",
    "![image.png](attachment:image.png)\n",
    "First of all, the location of the taxi (indicated by yello rectangle) is in the top row and third column, so the state should be between 40 to 59.\n",
    "Then we see that the location of the passenger (indicated by blue color) is in `G` cell, which shows that the sub-state must be ones of {4, 5, 6, 7}, **(e.g. {0, 1, 2, 3} are for the case when passenger is in `R` and destination is in four possible location; Please note the order of the locations in the above description)**. Since the destination (shown by Magenta color) is at `Y` cell, the total sub-state will be 4+2=6.\n",
    "\n",
    "Hence, the full state will be 40+4+2=46.\n",
    "\n",
    "Also note that you can get the current state via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the epsilon-greedy policy ü§ñ\n",
    "\n",
    "Epsilon-Greedy is a policy that handles the exploration/exploitation trade-off\n",
    "\n",
    "Epsilon Greedy:\n",
    "\n",
    "- *With probability 1‚Ää-‚Ää…õ*¬†: we do **exploitation** (aka our agent selects the action with the highest state-action pair value).\n",
    "\n",
    "- *With probability …õ*: we do **exploration** (trying random action).\n",
    "\n",
    "And as the training goes, **we progressively reduce the epsilon value** since we will **need less and less exploration and more exploitation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Q-Learning algorithm and train our agent üß†\n",
    "- Now we implement the Q learning algorithm:\n",
    "[Q-Learning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 25000        # Total number of training episodes\n",
    "total_test_episodes = 100     # Total number of test episodes\n",
    "action_str = {0: \"south\", 1: \"north\", 2: \"east\", 3: \"west\", 4: \"pickup\", 5: \"drop\"}\n",
    "\n",
    "class Qlearner:\n",
    "    def __init__(self, env):\n",
    "        # Create the Q-table and initialize it\n",
    "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.env = env\n",
    "        self.env.reset()\n",
    "        self.is_episode_over = False\n",
    "        self.learning_rate = 0.01  # Learning rate\n",
    "        self.gamma = 0.99  # Discounting rate\n",
    "        self.max_steps = 200  # Max steps per episode\n",
    "        self.epsilon = 1.0  # Exploration probability at start\n",
    "        self.max_epsilon = 1.0  # Exploration probability at start\n",
    "        self.min_epsilon = 0.001  # Minimum exploration probability \n",
    "        self.decay_rate = 0.01  # Exponential decay rate for exploration prob\n",
    "        \n",
    "    def get_epsilon_greedy_action(self):\n",
    "        z = random.uniform(0, 1)\n",
    "        if z <= self.epsilon:\n",
    "            action = np.random.choice(self.q_table.shape[1])\n",
    "            # OR: action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[self.env.s, :])\n",
    "        return action\n",
    "    \n",
    "    def train(self, total_episodes=total_episodes):\n",
    "        episode_reward = 0\n",
    "        pbar = tqdm.tqdm(range(total_episodes))\n",
    "        for episode in pbar:\n",
    "            if not (episode % 100):\n",
    "                pbar.set_description(f'sample episode_reward= {episode_reward}')\n",
    "            self.env.reset()\n",
    "            episode_reward = self.run_training_episode()\n",
    "            self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_rate * episode)\n",
    "            \n",
    "    def run_training_episode(self):\n",
    "        episode_reward = 0\n",
    "        for n in range(self.max_steps):\n",
    "            action = self.get_epsilon_greedy_action()\n",
    "            current_state = self.env.s\n",
    "            next_state, next_reward, is_episode_over, _ = self.env.step(action)\n",
    "            episode_reward += next_reward\n",
    "            next_state_max_value = self.q_table[next_state, :].max()\n",
    "            # Update the q_table\n",
    "            step = next_reward + self.gamma * next_state_max_value - self.q_table[current_state, action]\n",
    "            self.q_table[current_state, action] += self.learning_rate * step\n",
    "            if is_episode_over:\n",
    "                return episode_reward\n",
    "        return episode_reward\n",
    "    \n",
    "    def test(self, total_test_episodes=total_test_episodes):\n",
    "        for episode in range(total_test_episodes):\n",
    "            self.env.reset()\n",
    "            # self.env.render()\n",
    "            episode_reward = self.run_test_episode()\n",
    "            # self.env.render()\n",
    "            print(f'episode_reward = {episode_reward}')\n",
    "            print('---------------------------------\\n')\n",
    "            time.sleep(2)\n",
    "            \n",
    "    def run_test_episode(self):\n",
    "        episode_reward = 0\n",
    "        for _ in range(self.max_steps):\n",
    "            # Display the traveling taxi:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.q_table[self.env.s, :])\n",
    "            next_state, next_reward, is_episode_over, _ = self.env.step(action)\n",
    "            episode_reward += next_reward\n",
    "            if is_episode_over:\n",
    "                return episode_reward\n",
    "        return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learner Instatiation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlearner = Qlearner(env)\n",
    "qlearner.train(total_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlearner.test(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backup_tflow",
   "language": "python",
   "name": "backup_tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
